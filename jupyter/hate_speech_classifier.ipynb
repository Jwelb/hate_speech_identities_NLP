{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d936082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "# from transformers import * # this may screw things up later\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "# import emoji\n",
    "# import torch\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')      \n",
    "\n",
    "# Set CPU usage\n",
    "tf.config.threading.set_intra_op_parallelism_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29c1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if text == '':\n",
    "        return ''\n",
    "    else:\n",
    "        text = text.lower()\n",
    "        text_cleaned = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "        text_cleaned = re.sub(r'#[A-Za-z0-9_]+', '', text_cleaned)\n",
    "        text_cleaned = re.sub(r'https?:\\/\\/\\S*', '', text_cleaned)\n",
    "        text_cleaned = text_cleaned.replace(',', '')\n",
    "        \n",
    "        tokenized = nlp(text_cleaned)\n",
    "        output_list = []\n",
    "        for token in tokenized:\n",
    "            if not token.is_stop:\n",
    "                output_list.append(token.lemma_)\n",
    "        \n",
    "        output = ' '.join([x for x in output_list if x != ''])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e71014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_tokenizer_model(num_classes):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "    return bert_tokenizer, bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac397873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_embeddings(sentences, bert_tokenizer):\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent, add_special_tokens=True, max_length=64, pad_to_max_length=True,\n",
    "                                            return_attention_mask = True)\n",
    "        input_ids.append(bert_inp['input_ids'])\n",
    "        attention_masks.append(bert_inp['attention_mask'])\n",
    "        \n",
    "    input_ids=np.asarray(input_ids)\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "040b4829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 10:51:23.430335: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-07 10:51:23.430982: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-07 10:51:23.432435: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 19. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath='./output',save_weights_only=True,\n",
    "                                                monitor='val_loss',mode='min',save_best_only=True)]\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ba5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_fit_bert_model(bert_model, input_ids, attention_masks, labels, epochs):\n",
    "    bert_model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    bert_model.fit([input_ids, attention_masks], labels, batch_size=32,\n",
    "                       epochs=epochs, callbacks=callbacks)\n",
    "    return bert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9161a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Civility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3f75213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>label_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USER @USER You are an embarrassing citizen!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>embarrassing citizen ! !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER Seems hard to believe that you stood nex...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>hard believe stand guy wear short masturbate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@USER @USER @USER Wow !!! no wonder the Libera...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>wow ! ! ! wonder liberal get bad party bul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER @USER And not all idiots grandstands lik...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>idiot grandstand like</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER Bring on the hypocrite gungrabber. MAGA</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>bring hypocrite gungrabber . maga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label category  \\\n",
       "0      @USER @USER You are an embarrassing citizen!!   OFF      TIN   \n",
       "1  @USER Seems hard to believe that you stood nex...   OFF      TIN   \n",
       "2  @USER @USER @USER Wow !!! no wonder the Libera...   OFF      TIN   \n",
       "3  @USER @USER And not all idiots grandstands lik...   OFF      TIN   \n",
       "4      @USER Bring on the hypocrite gungrabber. MAGA   OFF      TIN   \n",
       "\n",
       "                                     preprocess_text  label_bin  \n",
       "0                           embarrassing citizen ! !          0  \n",
       "1    hard believe stand guy wear short masturbate...          0  \n",
       "2      wow ! ! ! wonder liberal get bad party bul...          0  \n",
       "3                              idiot grandstand like          0  \n",
       "4                  bring hypocrite gungrabber . maga          0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./civility_data/civility_data/train.tsv', sep='\\t', encoding='utf-8')\n",
    "df_train['preprocess_text'] = df_train['text'].apply(preprocess_text)\n",
    "df_train['label_bin'] = df_train['label'].apply(lambda x: 0 if x=='OFF' else 1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13a98268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_classes=len(df_train['label_bin'].unique())\n",
    "civ_bert_tokenizer, civ_bert_model = create_bert_tokenizer_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7c55b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10592, 10592)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = df_train['preprocess_text']\n",
    "civ_labels = df_train['label_bin']\n",
    "civ_labels = np.array(civ_labels)\n",
    "len(sentences), len(civ_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60d4e32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "civ_input_ids, civ_attention_masks = create_sentence_embeddings(sentences, civ_bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "842252f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9756WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "331/331 [==============================] - 46s 131ms/step - loss: 0.0647 - accuracy: 0.9756\n",
      "Epoch 2/5\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9873WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "331/331 [==============================] - 43s 131ms/step - loss: 0.0438 - accuracy: 0.9873\n",
      "Epoch 3/5\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9873WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "331/331 [==============================] - 44s 131ms/step - loss: 0.0377 - accuracy: 0.9873\n",
      "Epoch 4/5\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9895WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "331/331 [==============================] - 44s 133ms/step - loss: 0.0289 - accuracy: 0.9895\n",
      "Epoch 5/5\n",
      "331/331 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9921WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "331/331 [==============================] - 44s 132ms/step - loss: 0.0241 - accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "civ_bert_model = compile_fit_bert_model(civ_bert_model, civ_input_ids, civ_attention_masks, civ_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3529552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>perspective_score</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>label_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>0.311852</td>\n",
       "      <td>ask native americans .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>0.566334</td>\n",
       "      <td>home drunk ! ! !     👊 🇺 🇸 👊 url</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.110361</td>\n",
       "      <td>amazon investigate chinese employee sell inter...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>0.927032</td>\n",
       "      <td>should'vetaken \" piece shit volcano . 😂 \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.319764</td>\n",
       "      <td>obama want liberal &amp; amp ; illegal red state</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label category  \\\n",
       "0  @USER She should ask a few native Americans wh...   OFF      UNT   \n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...   OFF      TIN   \n",
       "2  Amazon is investigating Chinese employees who ...   NOT      NaN   \n",
       "3  @USER Someone should'veTaken\" this piece of sh...   OFF      UNT   \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...   NOT      NaN   \n",
       "\n",
       "   perspective_score                                    preprocess_text  \\\n",
       "0           0.311852                             ask native americans .   \n",
       "1           0.566334                   home drunk ! ! !     👊 🇺 🇸 👊 url   \n",
       "2           0.110361  amazon investigate chinese employee sell inter...   \n",
       "3           0.927032          should'vetaken \" piece shit volcano . 😂 \"   \n",
       "4           0.319764       obama want liberal & amp ; illegal red state   \n",
       "\n",
       "   label_bin  \n",
       "0          0  \n",
       "1          0  \n",
       "2          1  \n",
       "3          0  \n",
       "4          1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get civ dev data\n",
    "df_dev = pd.read_csv('./civility_data/civility_data/dev.tsv', sep='\\t', encoding='utf-8')\n",
    "df_dev['preprocess_text'] = df_dev['text'].apply(preprocess_text)\n",
    "df_dev['label_bin'] = df_dev['label'].apply(lambda x: 0 if x=='OFF' else 1)\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d099fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.815618</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>0.743630</td>\n",
       "      <td>0.767771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.850679</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>0.732158</td>\n",
       "      <td>0.771903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.641330</td>\n",
       "      <td>0.832780</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>0.737055</td>\n",
       "      <td>0.769156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>440.000000</td>\n",
       "      <td>884.000000</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>1324.000000</td>\n",
       "      <td>1324.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0           1  accuracy    macro avg  weighted avg\n",
       "precision    0.671642    0.815618  0.771903     0.743630      0.767771\n",
       "recall       0.613636    0.850679  0.771903     0.732158      0.771903\n",
       "f1-score     0.641330    0.832780  0.771903     0.737055      0.769156\n",
       "support    440.000000  884.000000  0.771903  1324.000000   1324.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_dev = df_dev['preprocess_text']\n",
    "labels_dev = np.array(df_dev['label_bin'])\n",
    "\n",
    "dev_input_ids, dev_attention_masks = create_sentence_embeddings(sentences_dev, civ_bert_tokenizer)\n",
    "preds = civ_bert_model.predict([dev_input_ids, dev_attention_masks], batch_size=32)\n",
    "pred_labels = preds['logits'].argmax(axis=1)\n",
    "df_dev['pred'] = pred_labels\n",
    "df_dev_classification = classification_report(df_dev['label_bin'].tolist(), df_dev['pred'].tolist(), output_dict=True)\n",
    "pd.DataFrame(df_dev_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c343a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1612ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# civ test data\n",
    "df_test = pd.read_csv('./civility_data/civility_data/test.tsv', sep='\\t', encoding='utf-8')\n",
    "df_test['preprocess_text'] = df_test['text'].apply(preprocess_text)\n",
    "sentences_test = df_test['preprocess_text']\n",
    "\n",
    "test_input_ids, test_attention_masks = create_sentence_embeddings(sentences_test, civ_bert_tokenizer)\n",
    "\n",
    "test_preds = bert_model.predict([input_ids_test, attention_masks_test],batch_size=32)\n",
    "test_pred_labels = test_preds['logits'].argmax(axis=1)\n",
    "df_test['label'] = test_pred_labels\n",
    "df_test.to_csv('./civility_data/civility_data/lynnette_ng_test_final.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f8c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac11450c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gab data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b87ee9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>purity</th>\n",
       "      <th>harm</th>\n",
       "      <th>im</th>\n",
       "      <th>cv</th>\n",
       "      <th>ex</th>\n",
       "      <th>degradation</th>\n",
       "      <th>fairness</th>\n",
       "      <th>hd</th>\n",
       "      <th>...</th>\n",
       "      <th>rel</th>\n",
       "      <th>sxo</th>\n",
       "      <th>rae</th>\n",
       "      <th>nat</th>\n",
       "      <th>pol</th>\n",
       "      <th>authority</th>\n",
       "      <th>vo</th>\n",
       "      <th>idl</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>preprocess_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29574085</td>\n",
       "      <td>People think bones are made of calcium  They a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>people think bone calcium     bone high calciu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37547580</td>\n",
       "      <td>Why hasn't this disgusting illegal Obama polic...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>disgusting illegal obama policy completely rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21542473</td>\n",
       "      <td>Persecution for righteousness' sake, part 2:  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>persecution righteousness ' sake 2 :</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26888094</td>\n",
       "      <td>Blasphemy will get you Everywhere.......</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>blasphemy .......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23041627</td>\n",
       "      <td>I don't see protecting our borders, as ALL oth...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>protect border nation ! have resemble martial ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    text_id                                               text  purity  harm  \\\n",
       "0  29574085  People think bones are made of calcium  They a...       0     0   \n",
       "1  37547580  Why hasn't this disgusting illegal Obama polic...       0     0   \n",
       "2  21542473  Persecution for righteousness' sake, part 2:  ...       0     0   \n",
       "3  26888094           Blasphemy will get you Everywhere.......       0     0   \n",
       "4  23041627  I don't see protecting our borders, as ALL oth...       0     0   \n",
       "\n",
       "   im  cv  ex  degradation  fairness  hd  ...  rel  sxo  rae  nat  pol  \\\n",
       "0   0   0   0            0         0   0  ...    0    0    0    0    0   \n",
       "1   0   0   0            0         0   0  ...    0    0    0    0    0   \n",
       "2   0   0   0            0         0   0  ...    0    0    0    0    0   \n",
       "3   0   0   0            1         0   0  ...    0    0    0    0    0   \n",
       "4   0   0   0            0         0   0  ...    0    0    0    0    0   \n",
       "\n",
       "   authority  vo  idl  label_bin  \\\n",
       "0          0   0    0          0   \n",
       "1          0   0    0          0   \n",
       "2          0   0    0          0   \n",
       "3          0   0    0          0   \n",
       "4          0   0    0          0   \n",
       "\n",
       "                                     preprocess_text  \n",
       "0  people think bone calcium     bone high calciu...  \n",
       "1  disgusting illegal obama policy completely rem...  \n",
       "2             persecution righteousness ' sake 2 :    \n",
       "3                                  blasphemy .......  \n",
       "4  protect border nation ! have resemble martial ...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gab_test = pd.read_csv('/storage2/mamille3/data/hate_speech/gab_hate_corpus//gab_test.tsv', sep='\\t')\n",
    "df_gab_test['label_bin'] = df_gab_test['vo'].apply(lambda x: 1 if x==1 else 0)\n",
    "df_gab_test['preprocess_text'] = df_gab_test['text'].apply(preprocess_text)\n",
    "df_gab_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1133139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentences_gab = df_gab_test['preprocess_text']\n",
    "input_ids_gab, attention_masks_gab = create_sentence_embeddings(sentences_gab, civ_bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ca3f947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.794562</td>\n",
       "      <td>0.015104</td>\n",
       "      <td>0.21495</td>\n",
       "      <td>0.404833</td>\n",
       "      <td>0.744752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.217625</td>\n",
       "      <td>0.175758</td>\n",
       "      <td>0.21495</td>\n",
       "      <td>0.196691</td>\n",
       "      <td>0.214950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.341669</td>\n",
       "      <td>0.027818</td>\n",
       "      <td>0.21495</td>\n",
       "      <td>0.184744</td>\n",
       "      <td>0.321613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>2417.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.21495</td>\n",
       "      <td>2582.000000</td>\n",
       "      <td>2582.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0           1  accuracy    macro avg  weighted avg\n",
       "precision     0.794562    0.015104   0.21495     0.404833      0.744752\n",
       "recall        0.217625    0.175758   0.21495     0.196691      0.214950\n",
       "f1-score      0.341669    0.027818   0.21495     0.184744      0.321613\n",
       "support    2417.000000  165.000000   0.21495  2582.000000   2582.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict with SemEval classifier \n",
    "gab_preds = civ_bert_model.predict([input_ids_gab, attention_masks_gab],batch_size=32)\n",
    "gab_pred_labels = gab_preds['logits'].argmax(axis=1)\n",
    "df_gab_test['label_pred'] = gab_pred_labels\n",
    "df_dev_classification = classification_report(df_gab_test['label_bin'].tolist(), df_gab_test['label_pred'].tolist(), output_dict=True)\n",
    "pd.DataFrame(df_dev_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8a1b2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Gab's own classifier\n",
    "df_gab_train = pd.read_csv('./gab_data/gab_data/gab_train.tsv', sep='\\t')\n",
    "df_gab_train['label_bin'] = df_gab_train['vo'].apply(lambda x: 1 if x==1 else 0)\n",
    "df_gab_train['preprocess_text'] = df_gab_train['text'].apply(preprocess_text)\n",
    "\n",
    "num_gab_classes=len(df_gab_train['label_bin'].unique())\n",
    "gab_bert_tokenizer, gab_bert_model = create_bert_tokenizer_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb1a8fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentences_gab_train = df_gab_train['preprocess_text']\n",
    "labels_gab_train = np.array(df_gab_train['label_bin'])\n",
    "\n",
    "input_ids_train_gab, attention_masks_train_gab = create_sentence_embeddings(sentences_gab_train, gab_bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "646/646 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.9553WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "646/646 [==============================] - 101s 145ms/step - loss: 0.2185 - accuracy: 0.9553\n",
      "Epoch 2/5\n",
      "646/646 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.9349WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "646/646 [==============================] - 94s 145ms/step - loss: 0.1779 - accuracy: 0.9349\n",
      "Epoch 3/5\n",
      "454/646 [====================>.........] - ETA: 27s - loss: 0.1914 - accuracy: 0.9341"
     ]
    }
   ],
   "source": [
    "gab_model = compile_fit_bert_model(gab_bert_model, input_ids_train_gab, attention_masks_train_gab, labels_gab_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "gab_new_preds = gab_model.predict([input_ids_gab, attention_masks_gab],batch_size=32)\n",
    "gab_new_pred_labels = gab_new_preds['logits'].argmax(axis=1)\n",
    "df_gab_test['label_pred_new'] = gab_new_pred_labels\n",
    "df_dev_classification = classification_report(df_gab_test['label_bin'].tolist(), df_gab_test['label_pred_new'].tolist(), output_dict=True)\n",
    "pd.DataFrame(df_dev_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af671175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ee7f22c",
   "metadata": {},
   "source": [
    "## Contextual Abuse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b40a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_contextual_test = pd.read_csv('contextual_abuse_dataset/data/data/cad_v1_1_test.tsv', sep='\\t')\n",
    "df_contextual_test = pd.read_csv('/storage2/mamille3/data/hate_speech/contextual_abuse_dataset/cad_v1_1_test.tsv', sep='\\t')\n",
    "def cad_off_or_not(label):\n",
    "    if label == 'Neutral':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "df_contextual_test = df_contextual_test.dropna(subset=['text'])\n",
    "df_contextual_test['label_bin'] = df_contextual_test['labels'].apply(cad_off_or_not)\n",
    "df_contextual_test['preprocess_text'] = df_contextual_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using SemEval model\n",
    "sentences_cad = df_contextual_test['preprocess_text']\n",
    "input_ids_cad, attention_masks_cad = create_sentence_embeddings(sentences_cad, civ_bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cad_preds = civ_bert_model.predict([input_ids_cad, attention_masks_cad],batch_size=32)\n",
    "cad_pred_labels = cad_preds['logits'].argmax(axis=1)\n",
    "df_contextual_test['label_pred'] = cad_pred_labels\n",
    "df_dev_classification = classification_report(df_contextual_test['label_bin'].tolist(), df_contextual_test['label_pred'].tolist(), output_dict=True)\n",
    "pd.DataFrame(df_dev_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ed1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAD own model\n",
    "# df_contextual_train = pd.read_csv('contextual_abuse_dataset/data/data/cad_v1_1_train.tsv', sep='\\t')\n",
    "df_contextual_train = pd.read_csv('/storage2/mamille3/data/hate_speech/contextual_abuse_dataset/cad_v1_1_train.tsv', sep='\\t')\n",
    "\n",
    "df_contextual_train = df_contextual_train.dropna(subset=['text'])\n",
    "df_contextual_train['label_bin'] = df_contextual_train['labels'].apply(cad_off_or_not) # did assign it to df_contextual_test (bug?)\n",
    "df_contextual_train['preprocess_text'] = df_contextual_train['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd2aabd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/mnt/cephfs/cluster/carley/carley02/mamille3/hegemonic_hate/conda_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_classes=len(df_contextual_train['label_bin'].unique()) # originally df_cad_train\n",
    "cad_bert_tokenizer, cad_bert_model = create_bert_tokenizer_model(num_classes)\n",
    "\n",
    "sentences_cad_train = df_contextual_train['preprocess_text']\n",
    "labels_cad_train = df_contextual_train['label_bin']\n",
    "\n",
    "input_ids_train_cad, attention_masks_train_cad = create_sentence_embeddings(sentences_cad_train, cad_bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f541ac97",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  6/425 [..............................] - ETA: 20:52 - loss: 0.5809 - accuracy: 0.6553"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18394/1142440680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcad_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_fit_bert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcad_bert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_train_cad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks_train_cad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_cad_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_18394/2841662427.py\u001b[0m in \u001b[0;36mcompile_fit_bert_model\u001b[0;34m(bert_model, input_ids, attention_masks, labels, epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile_fit_bert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     bert_model.fit([input_ids, attention_masks], labels, batch_size=32,\n\u001b[0m\u001b[1;32m      4\u001b[0m                        epochs=epochs, callbacks=callbacks)\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cephfs/cluster/carley/carley02/mamille3/hegemonic_hate/conda_env/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cephfs/cluster/carley/carley02/mamille3/hegemonic_hate/conda_env/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cephfs/cluster/carley/carley02/mamille3/hegemonic_hate/conda_env/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/mnt/cephfs/cluster/carley/carley02/mamille3/hegemonic_hate/conda_env/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cephfs/cluster/carley/carley02/mamille3/hegemonic_hate/conda_env/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/mnt/cephfs/cluster/carley/carley02/mamille3/hegemonic_hate/conda_env/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cephfs/cluster/carley/carley02/mamille3/hegemonic_hate/conda_env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cad_model = compile_fit_bert_model(cad_bert_model, input_ids_train_cad, attention_masks_train_cad, labels_cad_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07600590",
   "metadata": {},
   "outputs": [],
   "source": [
    "cad_new_preds = cad_model.predict([input_ids_cad, attention_masks_cad],batch_size=32)\n",
    "cad_new_pred_labels = cad_new_preds['logits'].argmax(axis=1)\n",
    "df_contextual_test['label_pred_new'] = cad_new_pred_labels\n",
    "df_dev_classification = classification_report(df_contextual_test['label_bin'].tolist(), df_contextual_test['label_pred_new'].tolist(), output_dict=True)\n",
    "pd.DataFrame(df_dev_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02999e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
