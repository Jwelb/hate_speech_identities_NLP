{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a5e9fe-64ef-46ce-897a-8df421731c11",
   "metadata": {},
   "source": [
    "# Load processed datasets\n",
    "From datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eebc3c51-57ed-406e-af69-4a3f7db47fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cad', 'elsherief2021', 'sbic', 'kennedy2020', 'salminen2018'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "path = '/storage2/mamille3/hegemonic_hate/tmp/processed_datasets.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    hate_datasets = pickle.load(f)\n",
    "hate_datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e1c50-6afe-4da8-b3e3-604a5710d728",
   "metadata": {},
   "source": [
    "# Split datasets\n",
    "Output: train, dev and test folds for with-hegemonic and no-hegemonic splits. All splits/folds have 30/70 hate/no-hate ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78cd7694-9865-4741-95b2-c817b3080d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cad\n",
      "with_heg\n",
      "train: 7587 instances\n",
      "marginalized    1288\n",
      "other            860\n",
      "hegemonic        181\n",
      "Name: group_label, dtype: int64\n",
      "False    0.693028\n",
      "True     0.306972\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "dev: 1265 instances\n",
      "marginalized    216\n",
      "other           116\n",
      "hegemonic        41\n",
      "Name: group_label, dtype: int64\n",
      "False    0.705138\n",
      "True     0.294862\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "test: 3794 instances\n",
      "marginalized    611\n",
      "other           391\n",
      "hegemonic        90\n",
      "Name: group_label, dtype: int64\n",
      "False    0.712177\n",
      "True     0.287823\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "********************************************\n",
      "elsherief2021\n",
      "with_heg\n",
      "train: 10393 instances\n",
      "marginalized    1874\n",
      "hegemonic        691\n",
      "other            565\n",
      "Name: group_label, dtype: int64\n",
      "False    0.698836\n",
      "True     0.301164\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "dev: 1733 instances\n",
      "marginalized    302\n",
      "hegemonic       118\n",
      "other            80\n",
      "Name: group_label, dtype: int64\n",
      "False    0.711483\n",
      "True     0.288517\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "test: 5197 instances\n",
      "marginalized    970\n",
      "hegemonic       352\n",
      "other           245\n",
      "Name: group_label, dtype: int64\n",
      "False    0.69848\n",
      "True     0.30152\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "********************************************\n",
      "kennedy2020\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31419/2485700057.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# print(len(resampled_no_heg))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_with_heg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresampled_no_heg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_with_heg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhegemonic_hate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;31m# print(len(intersection))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# print(len(hegemonic_hate))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "# for dataset in sorted(hate_datasets)[:1]:\n",
    "for dataset in sorted(hate_datasets):\n",
    "    print(dataset)\n",
    "\n",
    "    # Remove instances with hegemonic labels\n",
    "    # print(len(hate_datasets[dataset]))\n",
    "    # print(hate_datasets[dataset].group_label.unique())\n",
    "    no_hegemonic = hate_datasets[dataset].query('group_label != \"hegemonic\"')\n",
    "    # print(len(no_hegemonic))\n",
    "    # print(no_hegemonic.group_label.unique())\n",
    "\n",
    "    # Sample to specific ratios of hate/nonhate\n",
    "    hate_ratio = 0.30 # have to upsample nonhate from Kennedy+2020 if <0.3 and from Salminen+2018 if <0.7ish\n",
    "\n",
    "    # Desired sampling of non-hate. Keep all hate rows (for no_hegemonic since that's the smallest set)\n",
    "    n_hate = no_hegemonic.hate.sum()\n",
    "    # print(n_hate)\n",
    "    n_samples = {\n",
    "        True: n_hate,\n",
    "        False: int((n_hate*(1-hate_ratio))/hate_ratio)\n",
    "    }\n",
    "    # print(n_samples)\n",
    "    # print(no_hegemonic.hate.value_counts())\n",
    "    \n",
    "    def get_n_samples(x):\n",
    "        \"\"\" Get number of samples for a dataset split \"\"\"\n",
    "        desired_n = n_samples[x.name]\n",
    "        if desired_n > sum(x.hate==x.name): # if there are more rows needed than are present\n",
    "            return x.sample(desired_n, random_state=9, replace=True) # upsample nonhate\n",
    "        else:\n",
    "            return x.sample(desired_n, random_state=9, replace=False)\n",
    "\n",
    "    resampled_no_heg = no_hegemonic.groupby('hate').apply(get_n_samples)\n",
    "    resampled_no_heg.index = resampled_no_heg.index.droplevel('hate')\n",
    "    resampled_no_heg = resampled_no_heg.sample(frac=1, random_state=9)\n",
    "    # print(resampled_no_heg.hate.value_counts())\n",
    "    # print(resampled_no_heg.hate.value_counts(normalize=True))\n",
    "    # print(resampled_no_heg.group_label.value_counts())\n",
    "    # n_samples\n",
    "\n",
    "    # Sample with_hegemonic dataset\n",
    "    # Want to preserve all the hegemonic hate instances so take them out first, then add them back in\n",
    "    # Want to make this exactly the same as no_hegemonic, but with hegemonic hate instances replacing other hate\n",
    "    hegemonic_hate = hate_datasets[dataset].query('hate and group_label==\"hegemonic\"')\n",
    "    n_samples[True] = n_hate-len(hegemonic_hate)\n",
    "    no_hegemonic_hate = hate_datasets[dataset].query('not (hate and group_label==\"hegemonic\")')\n",
    "    # resampled_with_heg = no_hegemonic_hate.groupby('hate').apply(lambda x: x.sample(n_samples[x.name]))\n",
    "    resampled_with_heg = no_hegemonic_hate.groupby('hate').apply(get_n_samples)\n",
    "    resampled_with_heg.index = resampled_with_heg.index.droplevel('hate')\n",
    "    resampled_with_heg = pd.concat([resampled_with_heg, hegemonic_hate], axis=0) # add hegemonic back in\n",
    "    resampled_with_heg = resampled_with_heg.sample(frac=1, random_state=9)\n",
    "    # print(resampled_with_heg.hate.value_counts())\n",
    "    # print(resampled_with_heg.hate.value_counts(normalize=True))\n",
    "    # print(resampled_with_heg.group_label.value_counts())\n",
    "\n",
    "    # Test overlap between with_heg and no_heg to see if it's maximum that it can be\n",
    "    # Should be exact overlap on not hate\n",
    "    # print(len(resampled_with_heg))\n",
    "    # print(len(resampled_no_heg))\n",
    "    intersection = pd.merge(resampled_with_heg, resampled_no_heg)\n",
    "    assert len(resampled_with_heg) - len(intersection) == len(hegemonic_hate)\n",
    "    # print(len(intersection))\n",
    "    # print(len(hegemonic_hate))\n",
    "    with_heg_nonhate = resampled_with_heg.query('not hate')\n",
    "    no_heg_nonhate = resampled_no_heg.query('not hate')\n",
    "    # print(len(with_heg_nonhate))\n",
    "    # print(len(no_heg_nonhate))\n",
    "    nonhate_intersection = pd.merge(with_heg_nonhate, no_heg_nonhate) \n",
    "    assert len(with_heg_nonhate) == len(no_heg_nonhate) == len(nonhate_intersection)\n",
    "\n",
    "    # Split into train/dev/test 60/10/30\n",
    "    import numpy as np\n",
    "\n",
    "    with_heg, no_heg = {}, {}\n",
    "    with_heg['train'], with_heg['dev'], with_heg['test'] = np.split(resampled_with_heg, [int(0.6*len(resampled_with_heg)), int(0.7*len(resampled_with_heg))])\n",
    "    no_heg['train'], no_heg['dev'], no_heg['test'] = np.split(resampled_no_heg, [int(0.6*len(resampled_no_heg)), int(0.7*len(resampled_no_heg))])\n",
    "\n",
    "    print('with_heg')\n",
    "    for name, fold in with_heg.items():\n",
    "        print(f'{name}: {len(fold)} instances')\n",
    "        print(fold.group_label.value_counts())\n",
    "        print(fold.hate.value_counts(normalize=True))\n",
    "        # Test hate ratio\n",
    "        print()\n",
    "    # print('no_heg')\n",
    "    # for name, fold in no_heg.items():\n",
    "    #     print(f'{name}: {len(fold)} instances')\n",
    "    #     print(fold.group_label.value_counts())\n",
    "    \n",
    "    print('********************************************')\n",
    "\n",
    "    # Save out\n",
    "    # Just do /tmp for now, but when I settle on which splits are important, then save out to csvs in hate_speech/<dataset>\n",
    "    outpath = f'/storage2/mamille3/hegemonic_hate/tmp/{dataset}_hegsplits_{hate_ratio}hate.pkl'\n",
    "    with open(outpath, 'wb') as f:\n",
    "        pickle.dump({'with_heg': with_heg, 'no_heg': no_heg}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
