{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a5e9fe-64ef-46ce-897a-8df421731c11",
   "metadata": {},
   "source": [
    "# Load processed datasets\n",
    "From datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eebc3c51-57ed-406e-af69-4a3f7db47fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cad', 'elsherief2021', 'sbic', 'kennedy2020', 'salminen2018'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "path = '/storage2/mamille3/hegemonic_hate/tmp/processed_datasets.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    hate_datasets = pickle.load(f)\n",
    "hate_datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e1c50-6afe-4da8-b3e3-604a5710d728",
   "metadata": {},
   "source": [
    "# Split datasets\n",
    "Output: train, dev and test folds for with-hegemonic and no-hegemonic splits. All splits/folds have 30/70 hate/no-hate ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78cd7694-9865-4741-95b2-c817b3080d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729658e0c2e349cc9cc32275c425c706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cad\n",
      "with_heg\n",
      "train: 7587 instances\n",
      "marginalized    1244\n",
      "other            838\n",
      "hegemonic        182\n",
      "Name: group_label, dtype: int64\n",
      "False    0.701595\n",
      "True     0.298405\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "dev: 1265 instances\n",
      "marginalized    230\n",
      "other           128\n",
      "hegemonic        38\n",
      "Name: group_label, dtype: int64\n",
      "False    0.686957\n",
      "True     0.313043\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "test: 3794 instances\n",
      "marginalized    628\n",
      "other           414\n",
      "hegemonic        92\n",
      "Name: group_label, dtype: int64\n",
      "False    0.701107\n",
      "True     0.298893\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "********************************************\n",
      "elsherief2021\n",
      "with_heg\n",
      "train: 10393 instances\n",
      "marginalized    1904\n",
      "hegemonic        694\n",
      "other            517\n",
      "Name: group_label, dtype: int64\n",
      "False    0.700279\n",
      "True     0.299721\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "dev: 1733 instances\n",
      "marginalized    321\n",
      "hegemonic       106\n",
      "other            94\n",
      "Name: group_label, dtype: int64\n",
      "False    0.699365\n",
      "True     0.300635\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "test: 5197 instances\n",
      "marginalized    935\n",
      "hegemonic       361\n",
      "other           265\n",
      "Name: group_label, dtype: int64\n",
      "False    0.699634\n",
      "True     0.300366\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "********************************************\n",
      "kennedy2020\n",
      "with_heg\n",
      "train: 9901 instances\n",
      "marginalized    5261\n",
      "hegemonic       3746\n",
      "other            659\n",
      "Name: group_label, dtype: int64\n",
      "False    0.702656\n",
      "True     0.297344\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "dev: 1651 instances\n",
      "marginalized    861\n",
      "hegemonic       636\n",
      "other           121\n",
      "Name: group_label, dtype: int64\n",
      "False    0.688068\n",
      "True     0.311932\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "test: 4951 instances\n",
      "marginalized    2701\n",
      "hegemonic       1841\n",
      "other            314\n",
      "Name: group_label, dtype: int64\n",
      "False    0.698647\n",
      "True     0.301353\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "********************************************\n",
      "salminen2018\n",
      "with_heg\n",
      "train: 3739 instances\n",
      "other           597\n",
      "hegemonic       295\n",
      "marginalized    229\n",
      "Name: group_label, dtype: int64\n",
      "False    0.69992\n",
      "True     0.30008\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "dev: 624 instances\n",
      "other           110\n",
      "hegemonic        50\n",
      "marginalized     30\n",
      "Name: group_label, dtype: int64\n",
      "False    0.695513\n",
      "True     0.304487\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "test: 1870 instances\n",
      "other           310\n",
      "hegemonic       149\n",
      "marginalized     99\n",
      "Name: group_label, dtype: int64\n",
      "False    0.701604\n",
      "True     0.298396\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "********************************************\n",
      "sbic\n",
      "with_heg\n",
      "train: 40995 instances\n",
      "marginalized    7847\n",
      "other           1607\n",
      "hegemonic        297\n",
      "Name: group_label, dtype: int64\n",
      "False    0.699841\n",
      "True     0.300159\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "dev: 6833 instances\n",
      "marginalized    1324\n",
      "other            267\n",
      "hegemonic         62\n",
      "Name: group_label, dtype: int64\n",
      "False    0.700278\n",
      "True     0.299722\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "test: 20498 instances\n",
      "marginalized    3965\n",
      "other            788\n",
      "hegemonic        167\n",
      "Name: group_label, dtype: int64\n",
      "False    0.700215\n",
      "True     0.299785\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "********************************************\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for dataset in sorted(hate_datasets)[4:5]:\n",
    "for dataset in tqdm(sorted(hate_datasets)):\n",
    "    print(dataset)\n",
    "\n",
    "    # Remove instances with hegemonic labels\n",
    "    # print(len(hate_datasets[dataset]))\n",
    "    # print(hate_datasets[dataset].group_label.unique())\n",
    "    no_hegemonic = hate_datasets[dataset].query('group_label != \"hegemonic\"')\n",
    "    # print(len(no_hegemonic))\n",
    "    # print(no_hegemonic.group_label.unique())\n",
    "\n",
    "    # Sample to specific ratios of hate/nonhate\n",
    "    hate_ratio = 0.30\n",
    "\n",
    "    # Desired sampling of non-hate. Keep all hate rows (for no_hegemonic since that's the smallest set)\n",
    "    n_hate = no_hegemonic.hate.sum()\n",
    "    # print(n_hate)\n",
    "    n_samples = {\n",
    "        True: n_hate,\n",
    "        False: int((n_hate*(1-hate_ratio))/hate_ratio)\n",
    "    }\n",
    "    # print(n_samples)\n",
    "    # print(no_hegemonic.hate.value_counts())\n",
    "    \n",
    "    def get_n_samples(x):\n",
    "        \"\"\" Get number of samples for a dataset split \"\"\"\n",
    "        desired_n = n_samples[x.name]\n",
    "        if desired_n > sum(x.hate==x.name): # if there are more rows needed than are present\n",
    "            return x.sample(desired_n, random_state=9, replace=True) # upsample nonhate\n",
    "        else:\n",
    "            return x.sample(desired_n, random_state=9, replace=False)\n",
    "\n",
    "    resampled_no_heg = no_hegemonic.groupby('hate').apply(get_n_samples)\n",
    "    resampled_no_heg.index = resampled_no_heg.index.droplevel('hate')\n",
    "    resampled_no_heg = resampled_no_heg.sample(frac=1, random_state=9)\n",
    "    # print(resampled_no_heg.hate.value_counts())\n",
    "    # print(resampled_no_heg.hate.value_counts(normalize=True))\n",
    "    # print(resampled_no_heg.group_label.value_counts())\n",
    "    # n_samples\n",
    "\n",
    "    # Sample with_hegemonic dataset\n",
    "    # Want to preserve all the hegemonic instances (hate or non-hate) for maximum differences between datasets.\n",
    "    # So take them out first, then add them back in\n",
    "    # Want to make this exactly the same as no_hegemonic, but with hegemonic instances replacing others\n",
    "    hegemonic_hate = hate_datasets[dataset].query('hate and group_label==\"hegemonic\"')\n",
    "    # n_samples[True] = n_hate-len(hegemonic_hate)\n",
    "    hegemonic_nonhate = hate_datasets[dataset].query('(not hate) and (group_label==\"hegemonic\")')\n",
    "    # n_samples[False] = n_samples[False]-len(hegemonic_nonhate)\n",
    "    # no_hegemonic_hate = hate_datasets[dataset].query('not (hate and group_label==\"hegemonic\")')\n",
    "    # n_samples[True] = n_hate-len(hegemonic_hate)\n",
    "    # no_hegemonic_hate = hate_datasets[dataset].query('not (hate and group_label==\"hegemonic\")')\n",
    "    # resampled_with_heg = no_hegemonic_hate.groupby('hate').apply(lambda x: x.sample(n_samples[x.name]))\n",
    "    n_nonhate = len(resampled_no_heg.query('not hate'))\n",
    "    desired_n_hegemonic_nonhate = int(len(hegemonic_nonhate)/len(hate_datasets[dataset].query('not hate')) * n_samples[False]) # match ratio overall in the dataset\n",
    "    replacement = False\n",
    "    if desired_n_hegemonic_nonhate > len(hegemonic_nonhate): # have oversampled nonhate and some nonhate is labeled hegemonic\n",
    "        replacement = True\n",
    "    resampled_with_heg = pd.concat([resampled_no_heg.query('hate').sample(n_hate-len(hegemonic_hate)), \n",
    "                                    hegemonic_hate,\n",
    "                                    resampled_no_heg.query('not hate').sample(n_samples[False] - desired_n_hegemonic_nonhate),\n",
    "                                    hegemonic_nonhate.sample(desired_n_hegemonic_nonhate, replace=replacement)\n",
    "                                   ], axis=0)\n",
    "    \n",
    "    # resampled_with_heg = no_hegemonic_hate.groupby('hate').apply(get_n_samples)\n",
    "    # resampled_with_heg.index = resampled_with_heg.index.droplevel('hate')\n",
    "    # resampled_with_heg = pd.concat([resampled_with_heg, hegemonic_hate], axis=0) # add hegemonic back in\n",
    "    resampled_with_heg = resampled_with_heg.sample(frac=1, random_state=9)\n",
    "    # print(resampled_with_heg.hate.value_counts())\n",
    "    # print(resampled_with_heg.hate.value_counts(normalize=True))\n",
    "    # print(resampled_with_heg.group_label.value_counts())\n",
    "\n",
    "    # Test overlap between with_heg and no_heg to see if it's maximum that it can be\n",
    "    # Should be exact overlap on not hate that's not hegemonic\n",
    "    # print(len(resampled_with_heg))\n",
    "    # print(len(resampled_no_heg))\n",
    "    # intersection = set(resampled_with_heg.index).intersection(set(resampled_no_heg.index))\n",
    "    unique_heg = Counter(resampled_with_heg.index) - Counter(resampled_no_heg.index)\n",
    "    assert sum(unique_heg.values()) == len(hegemonic_hate) + desired_n_hegemonic_nonhate\n",
    "    # intersection = sorted(resampled_with_heg.index.tolist()) -\n",
    "    # assert (len(resampled_with_heg.index.drop_duplicates()) - len(intersection)) == (len(hegemonic_hate) + desired_n_hegemonic_nonhate)\n",
    "    # print(len(intersection))\n",
    "    # print(len(hegemonic_hate))\n",
    "    # with_heg_nonhate = resampled_with_heg.query('not hate')\n",
    "    # no_heg_nonhate = resampled_no_heg.query('not hate')\n",
    "    # # print(len(with_heg_nonhate))\n",
    "    # # print(len(no_heg_nonhate))\n",
    "    # nonhate_intersection = pd.merge(with_heg_nonhate, no_heg_nonhate) \n",
    "    # assert len(with_heg_nonhate) == len(no_heg_nonhate) == len(nonhate_intersection)\n",
    "\n",
    "    # Split into train/dev/test 60/10/30\n",
    "    import numpy as np\n",
    "\n",
    "    with_heg, no_heg = {}, {}\n",
    "    with_heg['train'], with_heg['dev'], with_heg['test'] = np.split(resampled_with_heg, [int(0.6*len(resampled_with_heg)), int(0.7*len(resampled_with_heg))])\n",
    "    no_heg['train'], no_heg['dev'], no_heg['test'] = np.split(resampled_no_heg, [int(0.6*len(resampled_no_heg)), int(0.7*len(resampled_no_heg))])\n",
    "\n",
    "    print('with_heg')\n",
    "    for name, fold in with_heg.items():\n",
    "        print(f'{name}: {len(fold)} instances')\n",
    "        print(fold.group_label.value_counts())\n",
    "        print(fold.hate.value_counts(normalize=True))\n",
    "        # Test hate ratio\n",
    "        print()\n",
    "    # print('no_heg')\n",
    "    # for name, fold in no_heg.items():\n",
    "    #     print(f'{name}: {len(fold)} instances')\n",
    "    #     print(fold.group_label.value_counts())\n",
    "    \n",
    "    print('********************************************')\n",
    "\n",
    "    # Save out\n",
    "    # Just do /tmp for now, but when I settle on which splits are important, then save out to csvs in hate_speech/<dataset>\n",
    "    outpath = f'/storage2/mamille3/hegemonic_hate/tmp/{dataset}_hegsplits_{hate_ratio}hate.pkl'\n",
    "    with open(outpath, 'wb') as f:\n",
    "        pickle.dump({'with_heg': with_heg, 'no_heg': no_heg}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
